{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 读取数据并分割数据\n",
    "# import pandas as pd\n",
    "# import os·\n",
    "# current_dir = os.getcwd()\n",
    "# data_dir = os.path.join(current_dir, 'data/weibo_senti_100k.csv')\n",
    "# df = pd.read_csv(data_dir)\n",
    "# import numpy as np\n",
    "# num_rows = len(df)\n",
    "# random_data = np.random.choice(np.arange(num_rows), num_rows, replace=False)\n",
    "# split_size = int(num_rows * 0.8)\n",
    "# df1 = df.iloc[random_data[:split_size],:]\n",
    "# df2 = df.iloc[random_data[split_size:],:]\n",
    "# df1.to_csv(os.path.join(current_dir,'./data/test.csv'),index=False)\n",
    "# df2.to_csv(os.path.join(current_dir,'./data/train.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T08:04:21.008594800Z",
     "start_time": "2023-11-14T08:04:20.703491800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9187</th>\n",
       "      <td>0</td>\n",
       "      <td>成都在召唤我[泪]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34075</th>\n",
       "      <td>1</td>\n",
       "      <td>大厨很给力~！哈哈哈?~   真没什么不能播的，但是千呼万唤始出来 真在说明压轴啊有木有~！...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59161</th>\n",
       "      <td>1</td>\n",
       "      <td>走漏眼，未去?！//@KC?味??：回复 @爱红酒的闷蛋:或者先吃吃（透支定先）再游也可以呀...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4353</th>\n",
       "      <td>1</td>\n",
       "      <td>我能说我是僵尸迷么  [哈哈] #360首发植物大战僵尸2#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65121</th>\n",
       "      <td>1</td>\n",
       "      <td>奶奶的，你终于给老子下来了！一身轻松！[嘻嘻][嘻嘻][哈哈][哈哈][太阳][太阳]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                             review\n",
       "9187       0                                          成都在召唤我[泪]\n",
       "34075      1  大厨很给力~！哈哈哈?~   真没什么不能播的，但是千呼万唤始出来 真在说明压轴啊有木有~！...\n",
       "59161      1  走漏眼，未去?！//@KC?味??：回复 @爱红酒的闷蛋:或者先吃吃（透支定先）再游也可以呀...\n",
       "4353       1                     我能说我是僵尸迷么  [哈哈] #360首发植物大战僵尸2#\n",
       "65121      1        奶奶的，你终于给老子下来了！一身轻松！[嘻嘻][嘻嘻][哈哈][哈哈][太阳][太阳]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取test数据\n",
    "import os\n",
    "import pandas as pd\n",
    "current_dir = os.getcwd()\n",
    "test_dir = os.path.join(current_dir, 'data/test.csv')\n",
    "df = pd.read_csv(test_dir)\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T10:38:39.537786400Z",
     "start_time": "2023-11-14T10:38:38.058261500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95990"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试先用小部分数据\n",
    "lens = int(len(df))\n",
    "sentences = df.review[0:lens]\n",
    "labels = df.label[0:lens]\n",
    "len(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T08:07:59.001296900Z",
     "start_time": "2023-11-14T08:07:46.923770Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-chinese/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D75B1CAA00>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: ee245a59-8f91-4696-a0be-75e3dc792662)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 回复@黛斯CHAN与小胖胖:亲…给点想象力好么？[汗]#与其相濡以沫…不如相望于江湖#[哈哈][哈哈][哈哈] //@黛斯CHAN与小胖胖:第五个菜名叫 尸首异处？？[晕][晕] //@懒人业余餐厅:我比较关心老板疯了没？@懒人业余餐厅厦门店\n",
      "Tokenized: ['回', '复', '@', '黛', '斯', '[UNK]', '与', '小', '胖', '胖', ':', '亲', '[UNK]', '给', '点', '想', '象', '力', '好', '么', '？', '[', '汗', ']', '#', '与', '其', '相', '濡', '以', '沫', '[UNK]', '不', '如', '相', '望', '于', '江', '湖', '#', '[', '哈', '哈', ']', '[', '哈', '哈', ']', '[', '哈', '哈', ']', '/', '/', '@', '黛', '斯', '[UNK]', '与', '小', '胖', '胖', ':', '第', '五', '个', '菜', '名', '叫', '尸', '首', '异', '处', '？', '？', '[', '晕', ']', '[', '晕', ']', '/', '/', '@', '懒', '人', '业', '余', '餐', '厅', ':', '我', '比', '较', '关', '心', '老', '板', '疯', '了', '没', '？', '@', '懒', '人', '业', '余', '餐', '厅', '厦', '门', '店']\n",
      "Token IDs: [1726, 1908, 137, 7950, 3172, 100, 680, 2207, 5523, 5523, 131, 779, 100, 5314, 4157, 2682, 6496, 1213, 1962, 720, 8043, 138, 3731, 140, 108, 680, 1071, 4685, 4091, 809, 3773, 100, 679, 1963, 4685, 3307, 754, 3736, 3959, 108, 138, 1506, 1506, 140, 138, 1506, 1506, 140, 138, 1506, 1506, 140, 120, 120, 137, 7950, 3172, 100, 680, 2207, 5523, 5523, 131, 5018, 758, 702, 5831, 1399, 1373, 2221, 7674, 2460, 1905, 8043, 8043, 138, 3238, 140, 138, 3238, 140, 120, 120, 137, 2750, 782, 689, 865, 7623, 1324, 131, 2769, 3683, 6772, 1068, 2552, 5439, 3352, 4556, 749, 3766, 8043, 137, 2750, 782, 689, 865, 7623, 1324, 1336, 7305, 2421]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "print('Original:',sentences[1])\n",
    "print('Tokenized:',tokenizer.tokenize(sentences[1]))\n",
    "print('Token IDs:',tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 195\n"
     ]
    }
   ],
   "source": [
    "# 查看句子最大长度\n",
    "max_len = 0\n",
    "for sentence in sentences:\n",
    "    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "print('Max sentence length:', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "d:\\Tools\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  下午，小憩一小时，由始至终做一梦，向@李峥嵘 同学汇报工作，达人啊，表格啊，带着微博去旅行啊。。[泪][泪][泪]\n",
      "Token IDs: tensor([ 101,  678, 1286, 8024, 2207, 2737,  671, 2207, 3198, 8024, 4507, 1993,\n",
      "        5635, 5303,  976,  671, 3457, 8024, 1403,  137, 3330, 2286, 2318, 1398,\n",
      "        2110, 3726, 2845, 2339,  868, 8024, 6809,  782, 1557, 8024, 6134, 3419,\n",
      "        1557, 8024, 2372, 4708, 2544, 1300, 1343, 3180, 6121, 1557,  511,  511,\n",
      "         138, 3801,  140,  138, 3801,  140,  138, 3801,  140,  102,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "Attention mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n",
      "Label: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个空列表，用于存储输入的句子\n",
    "input_ids =  []\n",
    "# 创建一个空列表，用于存储注意力掩码\n",
    "attention_masks = []\n",
    "# 遍历句子列表\n",
    "for sentence in sentences:\n",
    "    # 使用tokenizer对句子进行编码，并返回编码结果和注意力掩码\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens = True,\n",
    "        max_length = 64,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    # 将编码结果和注意力掩码添加到列表中\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# 将列表中的元素拼接在一起\n",
    "input_ids = torch.cat(input_ids,dim = 0)\n",
    "attention_masks = torch.cat(attention_masks,dim = 0)\n",
    "# 将标签转换为tensor格式\n",
    "labels = torch.tensor(labels)\n",
    "# 打印句子、编码结果、注意力掩码和标签\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "print('Attention mask:', attention_masks[0])\n",
    "print('Label:', labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67,193 training samples\n",
      "28,797 validation samples\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "# 将输入id，注意力掩码和标签封装成TensorDataset\n",
    "dataset = TensorDataset(input_ids,attention_masks,labels)\n",
    "# 计算训练集和验证集的大小\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# 将数据集按照训练集和验证集的比例划分\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "# 这个值最好取16或者32\n",
    "batch_szie = 16\n",
    "# 创建训练数据加载器\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    # 使用随机采样器，每次迭代都会返回一个新的随机序列\n",
    "    sampler = RandomSampler(train_dataset),\n",
    "    # 设置每个batch的大小\n",
    "    batch_size = batch_szie\n",
    ")\n",
    "# 创建验证数据加载器\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    # 使用顺序采样器，每次迭代都会返回一个新的序列\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    # 设置每个batch的大小\n",
    "    batch_size=batch_szie\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-chinese/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D75B6F8B80>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: b9e4a105-2179-44da-96df-6db838c42cdc)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-chinese/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D75B6F88B0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 976d69db-3f6e-475c-a1f6-13ca4196d41f)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "# 加载Bert预训练模型，并设置输出层数和类别数\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-chinese',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "# 将模型移动到GPU上\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Tools\\anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 定义优化器\n",
    "optimer = AdamW(model.parameters(),\n",
    "                lr = 5e-5,\n",
    "                eps = 1e-8)\n",
    "# 定义学习率调度器\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "epochs = 3\n",
    "total_steps =  len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# 定义一个函数，用于格式化时间\n",
    "def format_time(elapsed):\n",
    "    # 将时间elapsed四舍五入到秒\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # 返回格式化后的时间\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 根据预测结果和标签数据来计算准确率\n",
    "def flat_accuracy(preds, labels):\n",
    "    # 将预测结果转换为一维数组\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    # 将标签数据转换为一维数组\n",
    "    labels_flat = labels.flatten()\n",
    "    # 返回预测结果和标签数据的准确率\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  4,200.    Elapsed: 0:00:09.\n",
      "  Batch    80  of  4,200.    Elapsed: 0:00:14.\n",
      "  Batch   120  of  4,200.    Elapsed: 0:00:19.\n",
      "  Batch   160  of  4,200.    Elapsed: 0:00:24.\n",
      "  Batch   200  of  4,200.    Elapsed: 0:00:29.\n",
      "  Batch   240  of  4,200.    Elapsed: 0:00:35.\n",
      "  Batch   280  of  4,200.    Elapsed: 0:00:40.\n",
      "  Batch   320  of  4,200.    Elapsed: 0:00:45.\n",
      "  Batch   360  of  4,200.    Elapsed: 0:00:50.\n",
      "  Batch   400  of  4,200.    Elapsed: 0:00:55.\n",
      "  Batch   440  of  4,200.    Elapsed: 0:01:00.\n",
      "  Batch   480  of  4,200.    Elapsed: 0:01:05.\n",
      "  Batch   520  of  4,200.    Elapsed: 0:01:11.\n",
      "  Batch   560  of  4,200.    Elapsed: 0:01:16.\n",
      "  Batch   600  of  4,200.    Elapsed: 0:01:21.\n",
      "  Batch   640  of  4,200.    Elapsed: 0:01:26.\n",
      "  Batch   680  of  4,200.    Elapsed: 0:01:32.\n",
      "  Batch   720  of  4,200.    Elapsed: 0:01:37.\n",
      "  Batch   760  of  4,200.    Elapsed: 0:01:42.\n",
      "  Batch   800  of  4,200.    Elapsed: 0:01:47.\n",
      "  Batch   840  of  4,200.    Elapsed: 0:01:52.\n",
      "  Batch   880  of  4,200.    Elapsed: 0:01:58.\n",
      "  Batch   920  of  4,200.    Elapsed: 0:02:03.\n",
      "  Batch   960  of  4,200.    Elapsed: 0:02:08.\n",
      "  Batch 1,000  of  4,200.    Elapsed: 0:02:13.\n",
      "  Batch 1,040  of  4,200.    Elapsed: 0:02:19.\n",
      "  Batch 1,080  of  4,200.    Elapsed: 0:02:24.\n",
      "  Batch 1,120  of  4,200.    Elapsed: 0:02:29.\n",
      "  Batch 1,160  of  4,200.    Elapsed: 0:02:34.\n",
      "  Batch 1,200  of  4,200.    Elapsed: 0:02:39.\n",
      "  Batch 1,240  of  4,200.    Elapsed: 0:02:44.\n",
      "  Batch 1,280  of  4,200.    Elapsed: 0:02:49.\n",
      "  Batch 1,320  of  4,200.    Elapsed: 0:02:54.\n",
      "  Batch 1,360  of  4,200.    Elapsed: 0:03:00.\n",
      "  Batch 1,400  of  4,200.    Elapsed: 0:03:05.\n",
      "  Batch 1,440  of  4,200.    Elapsed: 0:03:10.\n",
      "  Batch 1,480  of  4,200.    Elapsed: 0:03:15.\n",
      "  Batch 1,520  of  4,200.    Elapsed: 0:03:20.\n",
      "  Batch 1,560  of  4,200.    Elapsed: 0:03:25.\n",
      "  Batch 1,600  of  4,200.    Elapsed: 0:03:30.\n",
      "  Batch 1,640  of  4,200.    Elapsed: 0:03:35.\n",
      "  Batch 1,680  of  4,200.    Elapsed: 0:03:40.\n",
      "  Batch 1,720  of  4,200.    Elapsed: 0:03:45.\n",
      "  Batch 1,760  of  4,200.    Elapsed: 0:03:50.\n",
      "  Batch 1,800  of  4,200.    Elapsed: 0:03:55.\n",
      "  Batch 1,840  of  4,200.    Elapsed: 0:04:00.\n",
      "  Batch 1,880  of  4,200.    Elapsed: 0:04:05.\n",
      "  Batch 1,920  of  4,200.    Elapsed: 0:04:10.\n",
      "  Batch 1,960  of  4,200.    Elapsed: 0:04:16.\n",
      "  Batch 2,000  of  4,200.    Elapsed: 0:04:21.\n",
      "  Batch 2,040  of  4,200.    Elapsed: 0:04:26.\n",
      "  Batch 2,080  of  4,200.    Elapsed: 0:04:31.\n",
      "  Batch 2,120  of  4,200.    Elapsed: 0:04:37.\n",
      "  Batch 2,160  of  4,200.    Elapsed: 0:04:42.\n",
      "  Batch 2,200  of  4,200.    Elapsed: 0:04:47.\n",
      "  Batch 2,240  of  4,200.    Elapsed: 0:04:52.\n",
      "  Batch 2,280  of  4,200.    Elapsed: 0:04:58.\n",
      "  Batch 2,320  of  4,200.    Elapsed: 0:05:03.\n",
      "  Batch 2,360  of  4,200.    Elapsed: 0:05:08.\n",
      "  Batch 2,400  of  4,200.    Elapsed: 0:05:13.\n",
      "  Batch 2,440  of  4,200.    Elapsed: 0:05:19.\n",
      "  Batch 2,480  of  4,200.    Elapsed: 0:05:24.\n",
      "  Batch 2,520  of  4,200.    Elapsed: 0:05:29.\n",
      "  Batch 2,560  of  4,200.    Elapsed: 0:05:34.\n",
      "  Batch 2,600  of  4,200.    Elapsed: 0:05:39.\n",
      "  Batch 2,640  of  4,200.    Elapsed: 0:05:45.\n",
      "  Batch 2,680  of  4,200.    Elapsed: 0:05:50.\n",
      "  Batch 2,720  of  4,200.    Elapsed: 0:05:55.\n",
      "  Batch 2,760  of  4,200.    Elapsed: 0:06:00.\n",
      "  Batch 2,800  of  4,200.    Elapsed: 0:06:05.\n",
      "  Batch 2,840  of  4,200.    Elapsed: 0:06:11.\n",
      "  Batch 2,880  of  4,200.    Elapsed: 0:06:16.\n",
      "  Batch 2,920  of  4,200.    Elapsed: 0:06:21.\n",
      "  Batch 2,960  of  4,200.    Elapsed: 0:06:26.\n",
      "  Batch 3,000  of  4,200.    Elapsed: 0:06:31.\n",
      "  Batch 3,040  of  4,200.    Elapsed: 0:06:36.\n",
      "  Batch 3,080  of  4,200.    Elapsed: 0:06:41.\n",
      "  Batch 3,120  of  4,200.    Elapsed: 0:06:46.\n",
      "  Batch 3,160  of  4,200.    Elapsed: 0:06:51.\n",
      "  Batch 3,200  of  4,200.    Elapsed: 0:06:57.\n",
      "  Batch 3,240  of  4,200.    Elapsed: 0:07:02.\n",
      "  Batch 3,280  of  4,200.    Elapsed: 0:07:07.\n",
      "  Batch 3,320  of  4,200.    Elapsed: 0:07:12.\n",
      "  Batch 3,360  of  4,200.    Elapsed: 0:07:17.\n",
      "  Batch 3,400  of  4,200.    Elapsed: 0:07:22.\n",
      "  Batch 3,440  of  4,200.    Elapsed: 0:07:27.\n",
      "  Batch 3,480  of  4,200.    Elapsed: 0:07:32.\n",
      "  Batch 3,520  of  4,200.    Elapsed: 0:07:38.\n",
      "  Batch 3,560  of  4,200.    Elapsed: 0:07:43.\n",
      "  Batch 3,600  of  4,200.    Elapsed: 0:07:48.\n",
      "  Batch 3,640  of  4,200.    Elapsed: 0:07:53.\n",
      "  Batch 3,680  of  4,200.    Elapsed: 0:07:58.\n",
      "  Batch 3,720  of  4,200.    Elapsed: 0:08:03.\n",
      "  Batch 3,760  of  4,200.    Elapsed: 0:08:08.\n",
      "  Batch 3,800  of  4,200.    Elapsed: 0:08:13.\n",
      "  Batch 3,840  of  4,200.    Elapsed: 0:08:19.\n",
      "  Batch 3,880  of  4,200.    Elapsed: 0:08:24.\n",
      "  Batch 3,920  of  4,200.    Elapsed: 0:08:29.\n",
      "  Batch 3,960  of  4,200.    Elapsed: 0:08:34.\n",
      "  Batch 4,000  of  4,200.    Elapsed: 0:08:39.\n",
      "  Batch 4,040  of  4,200.    Elapsed: 0:08:44.\n",
      "  Batch 4,080  of  4,200.    Elapsed: 0:08:49.\n",
      "  Batch 4,120  of  4,200.    Elapsed: 0:08:55.\n",
      "  Batch 4,160  of  4,200.    Elapsed: 0:09:00.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epcoh took: 0:09:05\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation Loss: 0.24\n",
      "  Validation took: 0:01:25\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  4,200.    Elapsed: 0:00:05.\n",
      "  Batch    80  of  4,200.    Elapsed: 0:00:10.\n",
      "  Batch   120  of  4,200.    Elapsed: 0:00:15.\n",
      "  Batch   160  of  4,200.    Elapsed: 0:00:21.\n",
      "  Batch   200  of  4,200.    Elapsed: 0:00:26.\n",
      "  Batch   240  of  4,200.    Elapsed: 0:00:31.\n",
      "  Batch   280  of  4,200.    Elapsed: 0:00:36.\n",
      "  Batch   320  of  4,200.    Elapsed: 0:00:41.\n",
      "  Batch   360  of  4,200.    Elapsed: 0:00:46.\n",
      "  Batch   400  of  4,200.    Elapsed: 0:00:51.\n",
      "  Batch   440  of  4,200.    Elapsed: 0:00:57.\n",
      "  Batch   480  of  4,200.    Elapsed: 0:01:02.\n",
      "  Batch   520  of  4,200.    Elapsed: 0:01:07.\n",
      "  Batch   560  of  4,200.    Elapsed: 0:01:12.\n",
      "  Batch   600  of  4,200.    Elapsed: 0:01:17.\n",
      "  Batch   640  of  4,200.    Elapsed: 0:01:22.\n",
      "  Batch   680  of  4,200.    Elapsed: 0:01:27.\n",
      "  Batch   720  of  4,200.    Elapsed: 0:01:33.\n",
      "  Batch   760  of  4,200.    Elapsed: 0:01:38.\n",
      "  Batch   800  of  4,200.    Elapsed: 0:01:43.\n",
      "  Batch   840  of  4,200.    Elapsed: 0:01:48.\n",
      "  Batch   880  of  4,200.    Elapsed: 0:01:53.\n",
      "  Batch   920  of  4,200.    Elapsed: 0:01:59.\n",
      "  Batch   960  of  4,200.    Elapsed: 0:02:04.\n",
      "  Batch 1,000  of  4,200.    Elapsed: 0:02:09.\n",
      "  Batch 1,040  of  4,200.    Elapsed: 0:02:14.\n",
      "  Batch 1,080  of  4,200.    Elapsed: 0:02:19.\n",
      "  Batch 1,120  of  4,200.    Elapsed: 0:02:24.\n",
      "  Batch 1,160  of  4,200.    Elapsed: 0:02:29.\n",
      "  Batch 1,200  of  4,200.    Elapsed: 0:02:35.\n",
      "  Batch 1,240  of  4,200.    Elapsed: 0:02:40.\n",
      "  Batch 1,280  of  4,200.    Elapsed: 0:02:45.\n",
      "  Batch 1,320  of  4,200.    Elapsed: 0:02:50.\n",
      "  Batch 1,360  of  4,200.    Elapsed: 0:02:55.\n",
      "  Batch 1,400  of  4,200.    Elapsed: 0:03:00.\n",
      "  Batch 1,440  of  4,200.    Elapsed: 0:03:06.\n",
      "  Batch 1,480  of  4,200.    Elapsed: 0:03:11.\n",
      "  Batch 1,520  of  4,200.    Elapsed: 0:03:16.\n",
      "  Batch 1,560  of  4,200.    Elapsed: 0:03:21.\n",
      "  Batch 1,600  of  4,200.    Elapsed: 0:03:26.\n",
      "  Batch 1,640  of  4,200.    Elapsed: 0:03:31.\n",
      "  Batch 1,680  of  4,200.    Elapsed: 0:03:37.\n",
      "  Batch 1,720  of  4,200.    Elapsed: 0:03:42.\n",
      "  Batch 1,760  of  4,200.    Elapsed: 0:03:47.\n",
      "  Batch 1,800  of  4,200.    Elapsed: 0:03:52.\n",
      "  Batch 1,840  of  4,200.    Elapsed: 0:03:57.\n",
      "  Batch 1,880  of  4,200.    Elapsed: 0:04:02.\n",
      "  Batch 1,920  of  4,200.    Elapsed: 0:04:08.\n",
      "  Batch 1,960  of  4,200.    Elapsed: 0:04:13.\n",
      "  Batch 2,000  of  4,200.    Elapsed: 0:04:18.\n",
      "  Batch 2,040  of  4,200.    Elapsed: 0:04:23.\n",
      "  Batch 2,080  of  4,200.    Elapsed: 0:04:28.\n",
      "  Batch 2,120  of  4,200.    Elapsed: 0:04:33.\n",
      "  Batch 2,160  of  4,200.    Elapsed: 0:04:39.\n",
      "  Batch 2,200  of  4,200.    Elapsed: 0:04:44.\n",
      "  Batch 2,240  of  4,200.    Elapsed: 0:04:49.\n",
      "  Batch 2,280  of  4,200.    Elapsed: 0:04:54.\n",
      "  Batch 2,320  of  4,200.    Elapsed: 0:04:59.\n",
      "  Batch 2,360  of  4,200.    Elapsed: 0:05:05.\n",
      "  Batch 2,400  of  4,200.    Elapsed: 0:05:10.\n",
      "  Batch 2,440  of  4,200.    Elapsed: 0:05:15.\n",
      "  Batch 2,480  of  4,200.    Elapsed: 0:05:20.\n",
      "  Batch 2,520  of  4,200.    Elapsed: 0:05:25.\n",
      "  Batch 2,560  of  4,200.    Elapsed: 0:05:31.\n",
      "  Batch 2,600  of  4,200.    Elapsed: 0:05:36.\n",
      "  Batch 2,640  of  4,200.    Elapsed: 0:05:41.\n",
      "  Batch 2,680  of  4,200.    Elapsed: 0:05:46.\n",
      "  Batch 2,720  of  4,200.    Elapsed: 0:05:51.\n",
      "  Batch 2,760  of  4,200.    Elapsed: 0:05:57.\n",
      "  Batch 2,800  of  4,200.    Elapsed: 0:06:02.\n",
      "  Batch 2,840  of  4,200.    Elapsed: 0:06:07.\n",
      "  Batch 2,880  of  4,200.    Elapsed: 0:06:12.\n",
      "  Batch 2,920  of  4,200.    Elapsed: 0:06:17.\n",
      "  Batch 2,960  of  4,200.    Elapsed: 0:06:22.\n",
      "  Batch 3,000  of  4,200.    Elapsed: 0:06:27.\n",
      "  Batch 3,040  of  4,200.    Elapsed: 0:06:32.\n",
      "  Batch 3,080  of  4,200.    Elapsed: 0:06:37.\n",
      "  Batch 3,120  of  4,200.    Elapsed: 0:06:42.\n",
      "  Batch 3,160  of  4,200.    Elapsed: 0:06:46.\n",
      "  Batch 3,200  of  4,200.    Elapsed: 0:06:51.\n",
      "  Batch 3,240  of  4,200.    Elapsed: 0:06:56.\n",
      "  Batch 3,280  of  4,200.    Elapsed: 0:07:01.\n",
      "  Batch 3,320  of  4,200.    Elapsed: 0:07:06.\n",
      "  Batch 3,360  of  4,200.    Elapsed: 0:07:11.\n",
      "  Batch 3,400  of  4,200.    Elapsed: 0:07:16.\n",
      "  Batch 3,440  of  4,200.    Elapsed: 0:07:21.\n",
      "  Batch 3,480  of  4,200.    Elapsed: 0:07:26.\n",
      "  Batch 3,520  of  4,200.    Elapsed: 0:07:31.\n",
      "  Batch 3,560  of  4,200.    Elapsed: 0:07:36.\n",
      "  Batch 3,600  of  4,200.    Elapsed: 0:07:41.\n",
      "  Batch 3,640  of  4,200.    Elapsed: 0:07:46.\n",
      "  Batch 3,680  of  4,200.    Elapsed: 0:07:51.\n",
      "  Batch 3,720  of  4,200.    Elapsed: 0:07:56.\n",
      "  Batch 3,760  of  4,200.    Elapsed: 0:08:01.\n",
      "  Batch 3,800  of  4,200.    Elapsed: 0:08:06.\n",
      "  Batch 3,840  of  4,200.    Elapsed: 0:08:11.\n",
      "  Batch 3,880  of  4,200.    Elapsed: 0:08:16.\n",
      "  Batch 3,920  of  4,200.    Elapsed: 0:08:21.\n",
      "  Batch 3,960  of  4,200.    Elapsed: 0:08:26.\n",
      "  Batch 4,000  of  4,200.    Elapsed: 0:08:31.\n",
      "  Batch 4,040  of  4,200.    Elapsed: 0:08:36.\n",
      "  Batch 4,080  of  4,200.    Elapsed: 0:08:41.\n",
      "  Batch 4,120  of  4,200.    Elapsed: 0:08:46.\n",
      "  Batch 4,160  of  4,200.    Elapsed: 0:08:51.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epcoh took: 0:08:56\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 0.26\n",
      "  Validation took: 0:01:23\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  4,200.    Elapsed: 0:00:05.\n",
      "  Batch    80  of  4,200.    Elapsed: 0:00:10.\n",
      "  Batch   120  of  4,200.    Elapsed: 0:00:15.\n",
      "  Batch   160  of  4,200.    Elapsed: 0:00:20.\n",
      "  Batch   200  of  4,200.    Elapsed: 0:00:25.\n",
      "  Batch   240  of  4,200.    Elapsed: 0:00:30.\n",
      "  Batch   280  of  4,200.    Elapsed: 0:00:35.\n",
      "  Batch   320  of  4,200.    Elapsed: 0:00:40.\n",
      "  Batch   360  of  4,200.    Elapsed: 0:00:45.\n",
      "  Batch   400  of  4,200.    Elapsed: 0:00:50.\n",
      "  Batch   440  of  4,200.    Elapsed: 0:00:55.\n",
      "  Batch   480  of  4,200.    Elapsed: 0:01:00.\n",
      "  Batch   520  of  4,200.    Elapsed: 0:01:05.\n",
      "  Batch   560  of  4,200.    Elapsed: 0:01:10.\n",
      "  Batch   600  of  4,200.    Elapsed: 0:01:15.\n",
      "  Batch   640  of  4,200.    Elapsed: 0:01:20.\n",
      "  Batch   680  of  4,200.    Elapsed: 0:01:25.\n",
      "  Batch   720  of  4,200.    Elapsed: 0:01:30.\n",
      "  Batch   760  of  4,200.    Elapsed: 0:01:35.\n",
      "  Batch   800  of  4,200.    Elapsed: 0:01:40.\n",
      "  Batch   840  of  4,200.    Elapsed: 0:01:45.\n",
      "  Batch   880  of  4,200.    Elapsed: 0:01:50.\n",
      "  Batch   920  of  4,200.    Elapsed: 0:01:55.\n",
      "  Batch   960  of  4,200.    Elapsed: 0:02:00.\n",
      "  Batch 1,000  of  4,200.    Elapsed: 0:02:05.\n",
      "  Batch 1,040  of  4,200.    Elapsed: 0:02:10.\n",
      "  Batch 1,080  of  4,200.    Elapsed: 0:02:15.\n",
      "  Batch 1,120  of  4,200.    Elapsed: 0:02:20.\n",
      "  Batch 1,160  of  4,200.    Elapsed: 0:02:25.\n",
      "  Batch 1,200  of  4,200.    Elapsed: 0:02:30.\n",
      "  Batch 1,240  of  4,200.    Elapsed: 0:02:35.\n",
      "  Batch 1,280  of  4,200.    Elapsed: 0:02:40.\n",
      "  Batch 1,320  of  4,200.    Elapsed: 0:02:45.\n",
      "  Batch 1,360  of  4,200.    Elapsed: 0:02:49.\n",
      "  Batch 1,400  of  4,200.    Elapsed: 0:02:54.\n",
      "  Batch 1,440  of  4,200.    Elapsed: 0:02:59.\n",
      "  Batch 1,480  of  4,200.    Elapsed: 0:03:04.\n",
      "  Batch 1,520  of  4,200.    Elapsed: 0:03:09.\n",
      "  Batch 1,560  of  4,200.    Elapsed: 0:03:14.\n",
      "  Batch 1,600  of  4,200.    Elapsed: 0:03:19.\n",
      "  Batch 1,640  of  4,200.    Elapsed: 0:03:24.\n",
      "  Batch 1,680  of  4,200.    Elapsed: 0:03:29.\n",
      "  Batch 1,720  of  4,200.    Elapsed: 0:03:34.\n",
      "  Batch 1,760  of  4,200.    Elapsed: 0:03:39.\n",
      "  Batch 1,800  of  4,200.    Elapsed: 0:03:44.\n",
      "  Batch 1,840  of  4,200.    Elapsed: 0:03:49.\n",
      "  Batch 1,880  of  4,200.    Elapsed: 0:03:54.\n",
      "  Batch 1,920  of  4,200.    Elapsed: 0:03:59.\n",
      "  Batch 1,960  of  4,200.    Elapsed: 0:04:04.\n",
      "  Batch 2,000  of  4,200.    Elapsed: 0:04:09.\n",
      "  Batch 2,040  of  4,200.    Elapsed: 0:04:14.\n",
      "  Batch 2,080  of  4,200.    Elapsed: 0:04:19.\n",
      "  Batch 2,120  of  4,200.    Elapsed: 0:04:24.\n",
      "  Batch 2,160  of  4,200.    Elapsed: 0:04:29.\n",
      "  Batch 2,200  of  4,200.    Elapsed: 0:04:34.\n",
      "  Batch 2,240  of  4,200.    Elapsed: 0:04:39.\n",
      "  Batch 2,280  of  4,200.    Elapsed: 0:04:44.\n",
      "  Batch 2,320  of  4,200.    Elapsed: 0:04:49.\n",
      "  Batch 2,360  of  4,200.    Elapsed: 0:04:54.\n",
      "  Batch 2,400  of  4,200.    Elapsed: 0:05:00.\n",
      "  Batch 2,440  of  4,200.    Elapsed: 0:05:04.\n",
      "  Batch 2,480  of  4,200.    Elapsed: 0:05:09.\n",
      "  Batch 2,520  of  4,200.    Elapsed: 0:05:14.\n",
      "  Batch 2,560  of  4,200.    Elapsed: 0:05:19.\n",
      "  Batch 2,600  of  4,200.    Elapsed: 0:05:24.\n",
      "  Batch 2,640  of  4,200.    Elapsed: 0:05:29.\n",
      "  Batch 2,680  of  4,200.    Elapsed: 0:05:34.\n",
      "  Batch 2,720  of  4,200.    Elapsed: 0:05:39.\n",
      "  Batch 2,760  of  4,200.    Elapsed: 0:05:44.\n",
      "  Batch 2,800  of  4,200.    Elapsed: 0:05:49.\n",
      "  Batch 2,840  of  4,200.    Elapsed: 0:05:54.\n",
      "  Batch 2,880  of  4,200.    Elapsed: 0:05:59.\n",
      "  Batch 2,920  of  4,200.    Elapsed: 0:06:04.\n",
      "  Batch 2,960  of  4,200.    Elapsed: 0:06:09.\n",
      "  Batch 3,000  of  4,200.    Elapsed: 0:06:15.\n",
      "  Batch 3,040  of  4,200.    Elapsed: 0:06:20.\n",
      "  Batch 3,080  of  4,200.    Elapsed: 0:06:24.\n",
      "  Batch 3,120  of  4,200.    Elapsed: 0:06:29.\n",
      "  Batch 3,160  of  4,200.    Elapsed: 0:06:35.\n",
      "  Batch 3,200  of  4,200.    Elapsed: 0:06:40.\n",
      "  Batch 3,240  of  4,200.    Elapsed: 0:06:45.\n",
      "  Batch 3,280  of  4,200.    Elapsed: 0:06:50.\n",
      "  Batch 3,320  of  4,200.    Elapsed: 0:06:55.\n",
      "  Batch 3,360  of  4,200.    Elapsed: 0:07:00.\n",
      "  Batch 3,400  of  4,200.    Elapsed: 0:07:05.\n",
      "  Batch 3,440  of  4,200.    Elapsed: 0:07:10.\n",
      "  Batch 3,480  of  4,200.    Elapsed: 0:07:15.\n",
      "  Batch 3,520  of  4,200.    Elapsed: 0:07:20.\n",
      "  Batch 3,560  of  4,200.    Elapsed: 0:07:25.\n",
      "  Batch 3,600  of  4,200.    Elapsed: 0:07:30.\n",
      "  Batch 3,640  of  4,200.    Elapsed: 0:07:35.\n",
      "  Batch 3,680  of  4,200.    Elapsed: 0:07:40.\n",
      "  Batch 3,720  of  4,200.    Elapsed: 0:07:45.\n",
      "  Batch 3,760  of  4,200.    Elapsed: 0:07:50.\n",
      "  Batch 3,800  of  4,200.    Elapsed: 0:07:55.\n",
      "  Batch 3,840  of  4,200.    Elapsed: 0:08:00.\n",
      "  Batch 3,880  of  4,200.    Elapsed: 0:08:05.\n",
      "  Batch 3,920  of  4,200.    Elapsed: 0:08:09.\n",
      "  Batch 3,960  of  4,200.    Elapsed: 0:08:14.\n",
      "  Batch 4,000  of  4,200.    Elapsed: 0:08:19.\n",
      "  Batch 4,040  of  4,200.    Elapsed: 0:08:24.\n",
      "  Batch 4,080  of  4,200.    Elapsed: 0:08:29.\n",
      "  Batch 4,120  of  4,200.    Elapsed: 0:08:34.\n",
      "  Batch 4,160  of  4,200.    Elapsed: 0:08:39.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epcoh took: 0:08:44\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 0.20\n",
      "  Validation took: 0:01:22\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:30:55 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_val = 42 \n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "# 开始训练\n",
    "for epoch_i in range(0,epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elasped = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elasped))\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "        loss, logits = model(b_input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=b_input_mask,\n",
    "                             labels=b_labels).loss,model(b_input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=b_input_mask,\n",
    "                             labels=b_labels).logits\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(b_input_ids,\n",
    "                                 token_type_ids=None,\n",
    "                                 attention_mask=b_input_mask,\n",
    "                                 labels=b_labels).loss,model(b_input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=b_input_mask,\n",
    "                             labels=b_labels).logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    # 统计本次 epoch 的 loss\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # 统计本次评估的时长\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # 记录本次 epoch 的所有统计信息\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to d:\\Project\\python\\deep learning\\Bert\\weibo_classification\\weibo_model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('d:\\\\Project\\\\python\\\\deep learning\\\\Bert\\\\weibo_classification\\\\weibo_model_save/tokenizer_config.json',\n",
       " 'd:\\\\Project\\\\python\\\\deep learning\\\\Bert\\\\weibo_classification\\\\weibo_model_save/special_tokens_map.json',\n",
       " 'd:\\\\Project\\\\python\\\\deep learning\\\\Bert\\\\weibo_classification\\\\weibo_model_save/vocab.txt',\n",
       " 'd:\\\\Project\\\\python\\\\deep learning\\\\Bert\\\\weibo_classification\\\\weibo_model_save/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型\n",
    "import os\n",
    "\n",
    "# 模型存储到的路径\n",
    "output_dir = os.path.join(current_dir,'weibo_model_save/')\n",
    "\n",
    "# 目录不存在则创建\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# 使用 `save_pretrained()` 来保存已训练的模型，模型配置和分词器\n",
    "# 它们后续可以通过 `from_pretrained()` 加载\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # 考虑到分布式/并行（distributed/parallel）训练\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.242259</td>\n",
       "      <td>0.235836</td>\n",
       "      <td>0.922804</td>\n",
       "      <td>0:09:05</td>\n",
       "      <td>0:01:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.265279</td>\n",
       "      <td>0.258744</td>\n",
       "      <td>0.927388</td>\n",
       "      <td>0:08:56</td>\n",
       "      <td>0:01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.202896</td>\n",
       "      <td>0.199578</td>\n",
       "      <td>0.932318</td>\n",
       "      <td>0:08:44</td>\n",
       "      <td>0:01:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1           0.242259     0.235836       0.922804       0:09:05         0:01:25\n",
       "2           0.265279     0.258744       0.927388       0:08:56         0:01:23\n",
       "3           0.202896     0.199578       0.932318       0:08:44         0:01:22"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练统计到 DataFrame 中\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# 使用 epoch 值作为每行的索引\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# 展示表格数据\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果概率： [0.9955440163612366, 0.004456047900021076]\n",
      "评论为负向评论，预测结果为：负向评论\n"
     ]
    }
   ],
   "source": [
    "# 预测一段评论\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# 加载预训练的BERT模型和微调后的二分类模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained('./weibo_model_save').to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('./weibo_model_save')\n",
    "\n",
    "# 对输入的句子进行分词和编码\n",
    "text = '''\n",
    "我！绝对！是！个！奇！葩！好一番折腾！千回百转「无数坎坷辛酸好么！！」终于买到了机票候机了！\n",
    "话说我穿着棉袄在冰冷的办公室忙活啊！今天龙抬头我准备理发的好么！忍了好多天了好么！所以想5点再飞！\n",
    "深圳！我披头散发穿着棉袄来了！感谢贵人七妙！同事说我靠奇迹和好运活到今天的！[泪][泪][泪]\n",
    "'''\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "# 使用BERT模型对输入进行编码\n",
    "with torch.no_grad():\n",
    "  outputs = model(input_ids)\n",
    "# 获取概率分布\n",
    "logits = outputs[0]\n",
    "probabilities = torch.softmax(logits, dim=-1).tolist()[0]\n",
    "\n",
    "# 输出预测结果\n",
    "print('预测结果概率：',probabilities)\n",
    "if(probabilities[0]>probabilities[1]):\n",
    "    print('评论为负向评论，预测结果为：负向评论')\n",
    "else:\n",
    "    print('评论为正向评论，预测结果为：正向评论')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
